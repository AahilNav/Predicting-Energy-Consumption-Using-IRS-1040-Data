---
title: "Model prediction"
author: "Aahil Navroz, Joseph Williams, Qi Suqian"
date: "`r Sys.Date()`"
output: html_document
---

### Modeling

Aahil Navroz, Joseph Williams, Qi Suqian

```{r, message=FALSE, echo=FALSE, warning=FALSE}
set.seed(17)
library(caret)
library(randomForest)
library(gbm)
library(xgboost)
library(yardstick)
library(Metrics)
library(ggplot2)
library(pdp)
library(dplyr)
```

```{r, echo=FALSE, message=FALSE, out.width="65%", fig.asp = 1.3, fig.align='center'}
# read the dataset
chicago_all = read.csv("../working_data/chicago_all.csv")

#only keep zip averages from IRS data
chicago_only = chicago_all[,1:29]
chicago_irs = chicago_all[,-(1:29)]
chicago_irs_filt = chicago_irs[, grepl("_av$", names(chicago_irs))]
chicago_all_filt = cbind(chicago_only, chicago_irs_filt)

#separate data by building subtype
chi_all = chicago_all_filt %>% filter(BUILDING_SUBTYPE=="All")
chi_only = chicago_only[,1:23] %>% filter(BUILDING_SUBTYPE=="All")

# choose the selected columns
cleaned_all = chi_all[,-c(1,2,3,8,9,10,11,12,13,14,15,16,17,19,24,25)]
cleaned_only = chi_only[,-c(1,2,3,8,9,10,11,12,13,14,15,16,17,19)]

#separate two target variables: THERMS.PER.SQFT and KWH.PER.SQFT
therm_all = cleaned_all[,-4]
kwh_all = cleaned_all[,-3]
therm_only = cleaned_only[,-4]
kwh_only = cleaned_only[,-3]
```




Modeling Code for THERM_ALL
```{r, message=FALSE, echo=FALSE, warning=FALSE}
##THERM MODELING_ALL
# split the dataset into train and test set
set.seed(17)
train_index_therm_all = createDataPartition(therm_all$THERMS.PER.SQFT, p = 0.7, list = FALSE)
train_therm_all = therm_all[train_index_therm_all, ]
test_therm_all = therm_all[-train_index_therm_all, ]

# Randomforest
rf_therm_all= randomForest(THERMS.PER.SQFT~ ., data = train_therm_all, ntree = 1000)

# GBM
trainControl_gbm_therm_all = trainControl(method = "cv",  
                               number = 10)    
gbm_grid = expand.grid(interaction.depth = 1:8,  
                          n.trees = 1000,          
                          shrinkage = c(0.001, 0.005, 0.01, 0.05, 0.1),
                        n.minobsinnode = c(4,6,8,10))
gbm_therm_all_train = train(THERMS.PER.SQFT ~ ., data = train_therm_all,
                    method = "gbm",
                    trControl = trainControl_gbm_therm_all,
                    tuneGrid = gbm_grid,
                    verbose = FALSE,
                    metric = "RMSE",  
                    distribution = "gaussian")
print(gbm_therm_all_train)
gbm_therm_all = gbm(THERMS.PER.SQFT ~ ., data = train_therm_all, distribution = "gaussian", 
                n.trees=1000,interaction.depth = gbm_therm_all_train$bestTune$interaction.depth,shrinkage = gbm_therm_all_train$bestTune$shrinkage,n.minobsinnode = gbm_therm_all_train$bestTune$n.minobsinnode)


# XGboost
xtherm_all = xgb.DMatrix(data=as.matrix(train_therm_all[,-3]),label=train_therm_all[,3])
xgb_grid = expand.grid(max_depth = c(4, 6, 8, 10),  # Example depths
                           subsample = c(0.5, 0.7, 0.9),  # Example subsample rates
                           eta = c(0.005, 0.01, 0.05, 0.1),  # Example learning rates
                           stringsAsFactors = FALSE)

# Initialize variables to store the best parameters and lowest RMSE
best_params_therm_all = list()
min_rmse_therm_all = Inf
 
# Loop through the grid
for(i in 1:nrow(xgb_grid)) {
   params = list(
     booster = "gbtree",
     max_depth = xgb_grid$max_depth[i],
     subsample = xgb_grid$subsample[i],
     eta = xgb_grid$eta[i]
   )
   
# Perform cross-validation
   cv = xgb.cv(
     params = params,
     data = xtherm_all,
     nrounds = 10000,
     nfold = 5,  
     watchlist = list(train = xtherm_all),
     early_stopping_rounds = 10,
     metrics = "rmse",
     maximize = FALSE  
   )
   
# Check if this model has the lowest RMSE so far
   if(cv$evaluation_log$test_rmse_mean[cv$best_iteration] < min_rmse_therm_all) {
     min_rmse_therm_all = cv$evaluation_log$test_rmse_mean[cv$best_iteration]
     best_params_therm_all = params
   }
 }
 
#print the best params
print(best_params_therm_all)

xgb_therm_all = xgb.train(booster = "gbtree", max_depth = best_params_therm_all$max_depth, subsample = best_params_therm_all$subsample, eta = best_params_therm_all$eta, data = xtherm_all,
                       nrounds = 10000, watchlist = list(train = xtherm_all),
                       early_stopping_rounds = 10,verbose = 0)

# compare performance of three tree models
ytherm_all = xgb.DMatrix(data=as.matrix(test_therm_all[,-3]),label=test_therm_all[,3])
rf_predict_therm_all = predict(rf_therm_all,newdata=test_therm_all[,-3])
gbm_predict_therm_all = predict(gbm_therm_all,newdata=test_therm_all[,-3])
xgb_predict_therm_all = predict(xgb_therm_all,newdata=ytherm_all)
therm_all_result = as.data.frame(cbind(test_therm_all$THERMS.PER.SQFT,rf_predict_therm_all,gbm_predict_therm_all,xgb_predict_therm_all))

rf_rmse_therm_all = yardstick::rmse(therm_all_result,V1,rf_predict_therm_all)
gbm_rmse_therm_all = yardstick::rmse(therm_all_result,V1,gbm_predict_therm_all)
xgb_rmse_therm_all = yardstick::rmse(therm_all_result,V1,xgb_predict_therm_all)

rf_mpe_therm_all = mpe(therm_all_result,V1,rf_predict_therm_all)
gbm_mpe_therm_all = mpe(therm_all_result,V1,gbm_predict_therm_all)
xgb_mpe_therm_all = mpe(therm_all_result,V1,xgb_predict_therm_all)

therm_all_rmse_value = c(rf_rmse_therm_all$.estimate,gbm_rmse_therm_all$.estimate,xgb_rmse_therm_all$.estimate)
therm_all_mpe_value = c(rf_mpe_therm_all$.estimate,gbm_mpe_therm_all$.estimate,xgb_mpe_therm_all$.estimate)

print(therm_all_rmse_value)
print(therm_all_mpe_value)

```

```{r, message=FALSE, echo=FALSE, warning=FALSE}
ggplot(mapping = aes(x = 1:nrow(therm_all_result))) +
  geom_point(aes(y = therm_all_result$V1), color = "black") +
  geom_point(aes(y = therm_all_result$rf_predict_therm_all), color = "lightblue") +
  geom_point(aes(y = therm_all_result$gbm_predict_therm_all), color = "lightgreen") +
  geom_point(aes(y = therm_all_result$xgb_predict_therm_all), color = "pink") +
  labs(title = "THERM.ALL Actual vs Model Performance for Test Set",
       x = "Row number", y = "Therm/Sqft",
       color = "Lines") +
  theme_minimal()
```


Modeling Code for KWH_ALL
```{r, message=FALSE, echo=FALSE, warning=FALSE}
##KWH MODELING_ALL
# split the dataset into train and test set
set.seed(17)
train_index_kwh_all = createDataPartition(kwh_all$KWH.PER.SQFT, p = 0.7, list = FALSE)
train_kwh_all = kwh_all[train_index_kwh_all, ]
test_kwh_all = kwh_all[-train_index_kwh_all, ]

# Randomforest
rf_kwh_all= randomForest(KWH.PER.SQFT~ ., data = train_kwh_all, ntree = 1000)

# GBM
trainControl_gbm_kwh_all = trainControl(method = "cv",  
                               number = 10)    
gbm_grid = expand.grid(interaction.depth = 1:8,  
                          n.trees = 1000,          
                          shrinkage = c(0.001, 0.005, 0.01, 0.05, 0.1),
                        n.minobsinnode = c(4,6,8,10))
gbm_kwh_all_train = train(KWH.PER.SQFT ~ ., data = train_kwh_all,
                    method = "gbm",
                    trControl = trainControl_gbm_kwh_all,
                    tuneGrid = gbm_grid,
                    verbose = FALSE,
                    metric = "RMSE",  
                    distribution = "gaussian")
print(gbm_kwh_all_train)
gbm_kwh_all = gbm(KWH.PER.SQFT ~ ., data = train_kwh_all, distribution = "gaussian", 
                n.trees=1000,interaction.depth = gbm_kwh_all_train$bestTune$interaction.depth,shrinkage = gbm_kwh_all_train$bestTune$shrinkage,n.minobsinnode = gbm_kwh_all_train$bestTune$n.minobsinnode)


# XGboost
xkwh_all = xgb.DMatrix(data=as.matrix(train_kwh_all[,-3]),label=train_kwh_all[,3])
xgb_grid = expand.grid(max_depth = c(4, 6, 8, 10),  # Example depths
                           subsample = c(0.5, 0.7, 0.9),  # Example subsample rates
                           eta = c(0.005, 0.01, 0.05, 0.1),  # Example learning rates
                           stringsAsFactors = FALSE)

# Initialize variables to store the best parameters and lowest RMSE
best_params_kwh_all = list()
min_rmse_kwh_all = Inf
 
# Loop through the grid
for(i in 1:nrow(xgb_grid)) {
   params = list(
     booster = "gbtree",
     max_depth = xgb_grid$max_depth[i],
     subsample = xgb_grid$subsample[i],
     eta = xgb_grid$eta[i]
   )
   
# Perform cross-validation
   cv = xgb.cv(
     params = params,
     data = xkwh_all,
     nrounds = 10000,
     nfold = 5,  
     watchlist = list(train = xkwh_all),
     early_stopping_rounds = 10,
     metrics = "rmse",
     maximize = FALSE  
   )
   
# Check if this model has the lowest RMSE so far
   if(cv$evaluation_log$test_rmse_mean[cv$best_iteration] < min_rmse_kwh_all) {
     min_rmse_kwh_all = cv$evaluation_log$test_rmse_mean[cv$best_iteration]
     best_params_kwh_all = params
   }
 }
 
#print the best params
print(best_params_kwh_all)

xgb_kwh_all = xgb.train(booster = "gbtree", max_depth = best_params_kwh_all$max_depth, subsample = best_params_kwh_all$subsample, eta = best_params_kwh_all$eta, data = xkwh_all,
                       nrounds = 10000, watchlist = list(train = xkwh_all),
                       early_stopping_rounds = 10,verbose = 0)

# compare performance of three tree models
ykwh_all = xgb.DMatrix(data=as.matrix(test_kwh_all[,-3]),label=test_kwh_all[,3])
rf_predict_kwh_all = predict(rf_kwh_all,newdata=test_kwh_all[,-3])
gbm_predict_kwh_all = predict(gbm_kwh_all,newdata=test_kwh_all[,-3])
xgb_predict_kwh_all = predict(xgb_kwh_all,newdata=ykwh_all)
kwh_all_result = as.data.frame(cbind(test_kwh_all$KWH.PER.SQFT,rf_predict_kwh_all,gbm_predict_kwh_all,xgb_predict_kwh_all))

rf_rmse_kwh_all = yardstick::rmse(kwh_all_result,V1,rf_predict_kwh_all)
gbm_rmse_kwh_all = yardstick::rmse(kwh_all_result,V1,gbm_predict_kwh_all)
xgb_rmse_kwh_all = yardstick::rmse(kwh_all_result,V1,xgb_predict_kwh_all)

rf_mpe_kwh_all = mpe(kwh_all_result,V1,rf_predict_kwh_all)
gbm_mpe_kwh_all = mpe(kwh_all_result,V1,gbm_predict_kwh_all)
xgb_mpe_kwh_all = mpe(kwh_all_result,V1,xgb_predict_kwh_all)

kwh_all_rmse_value = c(rf_rmse_kwh_all$.estimate,gbm_rmse_kwh_all$.estimate,xgb_rmse_kwh_all$.estimate)
kwh_all_mpe_value = c(rf_mpe_kwh_all$.estimate,gbm_mpe_kwh_all$.estimate,xgb_mpe_kwh_all$.estimate)

print(kwh_all_rmse_value)
print(kwh_all_mpe_value)

```

```{r, message=FALSE, echo=FALSE, warning=FALSE}
ggplot(mapping = aes(x = 1:nrow(kwh_all_result))) +
  geom_point(aes(y = kwh_all_result$V1), color = "black") +
  geom_point(aes(y = kwh_all_result$rf_predict_kwh_all), color = "lightblue") +
  geom_point(aes(y = kwh_all_result$gbm_predict_kwh_all), color = "lightgreen") +
  geom_point(aes(y = kwh_all_result$xgb_predict_kwh_all), color = "pink") +
  labs(title = "KWH.ALL Actual vs Model Performance for Test Set",
       x = "Row number", y = "KWH/Sqft",
       color = "Lines") +
  theme_minimal()
```





##OnlyChicagoData Modeling

Modeling Code for THERM_ONLY_CHICAGO_DATA
```{r, message=FALSE, echo=FALSE, warning=FALSE}
##THERM MODELING_ONLY_CHICAGO_DATA
# split the dataset into train and test set
set.seed(17)
train_index_therm_only = createDataPartition(therm_only$THERMS.PER.SQFT, p = 0.7, list = FALSE)
train_therm_only = therm_only[train_index_therm_only, ]
test_therm_only = therm_only[-train_index_therm_only, ]

# Randomforest
rf_therm_only= randomForest(THERMS.PER.SQFT~ ., data = train_therm_only, ntree = 1000)

# GBM
trainControl_gbm_therm_only = trainControl(method = "cv",  
                               number = 10)    
gbm_grid = expand.grid(interaction.depth = 1:8,  
                          n.trees = 1000,          
                          shrinkage = c(0.001, 0.005, 0.01, 0.05, 0.1),
                        n.minobsinnode = c(4,6,8,10))
gbm_therm_only_train = train(THERMS.PER.SQFT ~ ., data = train_therm_only,
                    method = "gbm",
                    trControl = trainControl_gbm_therm_only,
                    tuneGrid = gbm_grid,
                    verbose = FALSE,
                    metric = "RMSE",  
                    distribution = "gaussian")
print(gbm_therm_only_train)
gbm_therm_only = gbm(THERMS.PER.SQFT ~ ., data = train_therm_only, distribution = "gaussian", 
                n.trees=1000,interaction.depth = gbm_therm_only_train$bestTune$interaction.depth,shrinkage = gbm_therm_only_train$bestTune$shrinkage,n.minobsinnode = gbm_therm_only_train$bestTune$n.minobsinnode)


# XGboost
xtherm_only = xgb.DMatrix(data=as.matrix(train_therm_only[,-3]),label=train_therm_only[,3])
xgb_grid = expand.grid(max_depth = c(4, 6, 8, 10),  # Example depths
                           subsample = c(0.5, 0.7, 0.9),  # Example subsample rates
                           eta = c(0.005, 0.01, 0.05, 0.1),  # Example learning rates
                           stringsAsFactors = FALSE)

# Initialize variables to store the best parameters and lowest RMSE
best_params_therm_only = list()
min_rmse_therm_only = Inf
 
# Loop through the grid
for(i in 1:nrow(xgb_grid)) {
   params = list(
     booster = "gbtree",
     max_depth = xgb_grid$max_depth[i],
     subsample = xgb_grid$subsample[i],
     eta = xgb_grid$eta[i]
   )
   
# Perform cross-validation
   cv = xgb.cv(
     params = params,
     data = xtherm_only,
     nrounds = 10000,
     nfold = 5,  
     watchlist = list(train = xtherm_only),
     early_stopping_rounds = 10,
     metrics = "rmse",
     maximize = FALSE  
   )
   
# Check if this model has the lowest RMSE so far
   if(cv$evaluation_log$test_rmse_mean[cv$best_iteration] < min_rmse_therm_only) {
     min_rmse_therm_only = cv$evaluation_log$test_rmse_mean[cv$best_iteration]
     best_params_therm_only = params
   }
 }
 
#print the best params
print(best_params_therm_only)

xgb_therm_only = xgb.train(booster = "gbtree", max_depth = best_params_therm_only$max_depth, subsample = best_params_therm_only$subsample, eta = best_params_therm_only$eta, data = xtherm_only,
                       nrounds = 10000, watchlist = list(train = xtherm_only),
                       early_stopping_rounds = 10,verbose = 0)

# compare performance of three tree models
ytherm_only = xgb.DMatrix(data=as.matrix(test_therm_only[,-3]),label=test_therm_only[,3])
rf_predict_therm_only = predict(rf_therm_only,newdata=test_therm_only[,-3])
gbm_predict_therm_only = predict(gbm_therm_only,newdata=test_therm_only[,-3])
xgb_predict_therm_only = predict(xgb_therm_only,newdata=ytherm_only)
therm_only_result = as.data.frame(cbind(test_therm_only$THERMS.PER.SQFT,rf_predict_therm_only,gbm_predict_therm_only,xgb_predict_therm_only))

rf_rmse_therm_only = yardstick::rmse(therm_only_result,V1,rf_predict_therm_only)
gbm_rmse_therm_only = yardstick::rmse(therm_only_result,V1,gbm_predict_therm_only)
xgb_rmse_therm_only = yardstick::rmse(therm_only_result,V1,xgb_predict_therm_only)

rf_mpe_therm_only = mpe(therm_only_result,V1,rf_predict_therm_only)
gbm_mpe_therm_only = mpe(therm_only_result,V1,gbm_predict_therm_only)
xgb_mpe_therm_only = mpe(therm_only_result,V1,xgb_predict_therm_only)

therm_only_rmse_value = c(rf_rmse_therm_only$.estimate,gbm_rmse_therm_only$.estimate,xgb_rmse_therm_only$.estimate)
therm_only_mpe_value = c(rf_mpe_therm_only$.estimate,gbm_mpe_therm_only$.estimate,xgb_mpe_therm_only$.estimate)

print(therm_only_rmse_value)
print(therm_only_mpe_value)

```

```{r, message=FALSE, echo=FALSE, warning=FALSE}
ggplot(mapping = aes(x = 1:nrow(therm_only_result))) +
  geom_point(aes(y = therm_only_result$V1), color = "black") +
  geom_point(aes(y = therm_only_result$rf_predict_therm_only), color = "lightblue") +
  geom_point(aes(y = therm_only_result$gbm_predict_therm_only), color = "lightgreen") +
  geom_point(aes(y = therm_only_result$xgb_predict_therm_only), color = "pink") +
  labs(title = "THERM.ONLY_CHICAGO_DATA Actual vs Model Performance for Test Set",
       x = "Row number", y = "Therm/Sqft",
       color = "Lines") +
  theme_minimal()
```




Modeling code for KWH_CHICAGO_ONLY
```{r, message=FALSE, echo=FALSE, warning=FALSE}
##KWH MODELING_ONLY
# split the dataset into train and test set
set.seed(17)
train_index_kwh_only = createDataPartition(kwh_only$KWH.PER.SQFT, p = 0.7, list = FALSE)
train_kwh_only = kwh_only[train_index_kwh_only, ]
test_kwh_only = kwh_only[-train_index_kwh_only, ]

# Randomforest
rf_kwh_only= randomForest(KWH.PER.SQFT~ ., data = train_kwh_only, ntree = 1000)

# GBM
trainControl_gbm_kwh_only = trainControl(method = "cv",  
                               number = 10)    
gbm_grid = expand.grid(interaction.depth = 1:8,  
                          n.trees = 1000,          
                          shrinkage = c(0.001, 0.005, 0.01, 0.05, 0.1),
                        n.minobsinnode = c(4,6,8,10))
gbm_kwh_only_train = train(KWH.PER.SQFT ~ ., data = train_kwh_only,
                    method = "gbm",
                    trControl = trainControl_gbm_kwh_only,
                    tuneGrid = gbm_grid,
                    verbose = FALSE,
                    metric = "RMSE",  
                    distribution = "gaussian")
print(gbm_kwh_only_train)
gbm_kwh_only = gbm(KWH.PER.SQFT ~ ., data = train_kwh_only, distribution = "gaussian", 
                n.trees=1000,interaction.depth = gbm_kwh_only_train$bestTune$interaction.depth,shrinkage = gbm_kwh_only_train$bestTune$shrinkage,n.minobsinnode = gbm_kwh_only_train$bestTune$n.minobsinnode)


# XGboost
xkwh_only = xgb.DMatrix(data=as.matrix(train_kwh_only[,-3]),label=train_kwh_only[,3])
xgb_grid = expand.grid(max_depth = c(4, 6, 8, 10),  # Example depths
                           subsample = c(0.5, 0.7, 0.9),  # Example subsample rates
                           eta = c(0.005, 0.01, 0.05, 0.1),  # Example learning rates
                           stringsAsFactors = FALSE)

# Initialize variables to store the best parameters and lowest RMSE
best_params_kwh_only = list()
min_rmse_kwh_only = Inf
 
# Loop through the grid
for(i in 1:nrow(xgb_grid)) {
   params = list(
     booster = "gbtree",
     max_depth = xgb_grid$max_depth[i],
     subsample = xgb_grid$subsample[i],
     eta = xgb_grid$eta[i]
   )
   
# Perform cross-validation
   cv = xgb.cv(
     params = params,
     data = xkwh_only,
     nrounds = 10000,
     nfold = 5,  
     watchlist = list(train = xkwh_only),
     early_stopping_rounds = 10,
     metrics = "rmse",
     maximize = FALSE  
   )
   
# Check if this model has the lowest RMSE so far
   if(cv$evaluation_log$test_rmse_mean[cv$best_iteration] < min_rmse_kwh_only) {
     min_rmse_kwh_only = cv$evaluation_log$test_rmse_mean[cv$best_iteration]
     best_params_kwh_only = params
   }
 }
 
#print the best params
print(best_params_kwh_only)

xgb_kwh_only = xgb.train(booster = "gbtree", max_depth = best_params_kwh_only$max_depth, subsample = best_params_kwh_only$subsample, eta = best_params_kwh_only$eta, data = xkwh_only,
                       nrounds = 10000, watchlist = list(train = xkwh_only),
                       early_stopping_rounds = 10,verbose = 0)

# compare performance of three tree models
ykwh_only = xgb.DMatrix(data=as.matrix(test_kwh_only[,-3]),label=test_kwh_only[,3])
rf_predict_kwh_only = predict(rf_kwh_only,newdata=test_kwh_only[,-3])
gbm_predict_kwh_only = predict(gbm_kwh_only,newdata=test_kwh_only[,-3])
xgb_predict_kwh_only = predict(xgb_kwh_only,newdata=ykwh_only)
kwh_only_result = as.data.frame(cbind(test_kwh_only$KWH.PER.SQFT,rf_predict_kwh_only,gbm_predict_kwh_only,xgb_predict_kwh_only))

rf_rmse_kwh_only = yardstick::rmse(kwh_only_result,V1,rf_predict_kwh_only)
gbm_rmse_kwh_only = yardstick::rmse(kwh_only_result,V1,gbm_predict_kwh_only)
xgb_rmse_kwh_only = yardstick::rmse(kwh_only_result,V1,xgb_predict_kwh_only)

rf_mpe_kwh_only = mpe(kwh_only_result,V1,rf_predict_kwh_only)
gbm_mpe_kwh_only = mpe(kwh_only_result,V1,gbm_predict_kwh_only)
xgb_mpe_kwh_only = mpe(kwh_only_result,V1,xgb_predict_kwh_only)

kwh_only_rmse_value = c(rf_rmse_kwh_only$.estimate,gbm_rmse_kwh_only$.estimate,xgb_rmse_kwh_only$.estimate)
kwh_only_mpe_value = c(rf_mpe_kwh_only$.estimate,gbm_mpe_kwh_only$.estimate,xgb_mpe_kwh_only$.estimate)

print(kwh_only_rmse_value)
print(kwh_only_mpe_value)

```

```{r, message=FALSE, echo=FALSE, warning=FALSE}
ggplot(mapping = aes(x = 1:nrow(kwh_only_result))) +
  geom_point(aes(y = kwh_only_result$V1), color = "black") +
  geom_point(aes(y = kwh_only_result$rf_predict_kwh_only), color = "lightblue") +
  geom_point(aes(y = kwh_only_result$gbm_predict_kwh_only), color = "lightgreen") +
  geom_point(aes(y = kwh_only_result$xgb_predict_kwh_only), color = "pink") +
  labs(title = "KWH.ONLY_CHICAGO_DATA Actual vs Model Performance for Test Set",
       x = "Row number", y = "KWH/Sqft",
       color = "Lines") +
  theme_minimal()
```



