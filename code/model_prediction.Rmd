---
title: "Model prediction"
author: "Aahil Navroz, Joseph Williams, Qi Suqian"
date: "`r Sys.Date()`"
output: html_document
---

### Modeling

Aahil Navroz, Joseph Williams, Qi Suqian

```{r, message=FALSE, echo=FALSE, warning=FALSE}
set.seed(17)
library(caret)
library(randomForest)
library(gbm)
library(xgboost)
library(yardstick)
library(Metrics)
library(ggplot2)
library(dplyr)
library(knitr)
```

```{r, echo=FALSE, message=FALSE, out.width="65%", fig.asp = 1.3, fig.align='center'}
# read the dataset
chicago_all = read.csv("../working_data/chicago_all.csv")

#only keep zip averages from IRS data
chicago_only = chicago_all[,1:29]
chicago_irs = chicago_all[,-(1:29)]
chicago_irs_filt = chicago_irs[, grepl("_av$", names(chicago_irs))]
chicago_all_filt = cbind(chicago_only, chicago_irs_filt)

#separate data by building subtype
chi_all = chicago_all_filt %>% filter(BUILDING_SUBTYPE=="All")
chi_only = chicago_only[,1:23] %>% filter(BUILDING_SUBTYPE=="All")

# choose the selected columns
cleaned_all = chi_all[,-c(1,2,3,8,9,10,11,12,13,14,15,16,17,19,24,25)]
cleaned_only = chi_only[,-c(1,2,3,8,9,10,11,12,13,14,15,16,17,19)]

#separate two target variables: THERMS.PER.SQFT and KWH.PER.SQFT
therm_all = cleaned_all[,-4]
kwh_all = cleaned_all[,-3]
therm_only = cleaned_only[,-4]
kwh_only = cleaned_only[,-3]
```




Modeling Code for THERM_ALL
```{r, message=FALSE, echo=FALSE, warning=FALSE}
##THERM MODELING_ALL
# split the dataset into train and test set
set.seed(17)
train_index_therm_all = createDataPartition(therm_all$THERMS.PER.SQFT, p = 0.7, list = FALSE)
train_therm_all = therm_all[train_index_therm_all, ]
test_therm_all = therm_all[-train_index_therm_all, ]

# Baseline linear model
base_therm_all = lm(THERMS.PER.SQFT~ ., data = train_therm_all)

# Randomforest
rf_therm_all= randomForest(THERMS.PER.SQFT~ ., data = train_therm_all, ntree = 1000)

# GBM
trainControl_gbm_therm_all = trainControl(method = "cv",  
                               number = 10)    
gbm_grid = expand.grid(interaction.depth = 1:8,  
                          n.trees = 1000,          
                          shrinkage = c(0.001, 0.005, 0.01, 0.05, 0.1),
                        n.minobsinnode = c(4,6,8,10))
gbm_therm_all_train = train(THERMS.PER.SQFT ~ ., data = train_therm_all,
                    method = "gbm",
                    trControl = trainControl_gbm_therm_all,
                    tuneGrid = gbm_grid,
                    verbose = FALSE,
                    metric = "RMSE",  
                    distribution = "gaussian")
gbm_therm_all = gbm(THERMS.PER.SQFT ~ ., data = train_therm_all, distribution = "gaussian", 
                n.trees=1000,interaction.depth = gbm_therm_all_train$bestTune$interaction.depth,shrinkage = gbm_therm_all_train$bestTune$shrinkage,n.minobsinnode = gbm_therm_all_train$bestTune$n.minobsinnode)


# XGboost
xtherm_all = xgb.DMatrix(data=as.matrix(train_therm_all[,-3]),label=train_therm_all[,3])
xgb_grid = expand.grid(max_depth = c(4, 6, 8, 10),  # Example depths
                           subsample = c(0.5, 0.7, 0.9),  # Example subsample rates
                           eta = c(0.005, 0.01, 0.05, 0.1),  # Example learning rates
                           stringsAsFactors = FALSE)

# Initialize variables to store the best parameters and lowest RMSE
best_params_therm_all = list()
min_rmse_therm_all = Inf
 
# Loop through the grid
for(i in 1:nrow(xgb_grid)) {
   params = list(
     booster = "gbtree",
     max_depth = xgb_grid$max_depth[i],
     subsample = xgb_grid$subsample[i],
     eta = xgb_grid$eta[i]
   )
   
# Perform cross-validation
   cv = xgb.cv(
     params = params,
     data = xtherm_all,
     nrounds = 10000,
     nfold = 5,  
     watchlist = list(train = xtherm_all),
     early_stopping_rounds = 10,
     metrics = "rmse",
     maximize = FALSE  
   )
   
# Check if this model has the lowest RMSE so far
   if(cv$evaluation_log$test_rmse_mean[cv$best_iteration] < min_rmse_therm_all) {
     min_rmse_therm_all = cv$evaluation_log$test_rmse_mean[cv$best_iteration]
     best_params_therm_all = params
   }
 }
 

xgb_therm_all = xgb.train(booster = "gbtree", max_depth = best_params_therm_all$max_depth, subsample = best_params_therm_all$subsample, eta = best_params_therm_all$eta, data = xtherm_all,
                       nrounds = 10000, watchlist = list(train = xtherm_all),
                       early_stopping_rounds = 10,verbose = 0)

# compare performance of three tree models
ytherm_all = xgb.DMatrix(data=as.matrix(test_therm_all[,-3]),label=test_therm_all[,3])
base_predict_therm_all = predict(base_therm_all,newdata=test_therm_all[,-3])
rf_predict_therm_all = predict(rf_therm_all,newdata=test_therm_all[,-3])
gbm_predict_therm_all = predict(gbm_therm_all,newdata=test_therm_all[,-3])
xgb_predict_therm_all = predict(xgb_therm_all,newdata=ytherm_all)
therm_all_result = as.data.frame(cbind(test_therm_all$THERMS.PER.SQFT,base_predict_therm_all,rf_predict_therm_all,gbm_predict_therm_all,xgb_predict_therm_all))

base_rmse_therm_all = yardstick::rmse(therm_all_result,V1,base_predict_therm_all)
rf_rmse_therm_all = yardstick::rmse(therm_all_result,V1,rf_predict_therm_all)
gbm_rmse_therm_all = yardstick::rmse(therm_all_result,V1,gbm_predict_therm_all)
xgb_rmse_therm_all = yardstick::rmse(therm_all_result,V1,xgb_predict_therm_all)

base_mpe_therm_all = mpe(therm_all_result,V1,base_predict_therm_all)
rf_mpe_therm_all = mpe(therm_all_result,V1,rf_predict_therm_all)
gbm_mpe_therm_all = mpe(therm_all_result,V1,gbm_predict_therm_all)
xgb_mpe_therm_all = mpe(therm_all_result,V1,xgb_predict_therm_all)

therm_all_rmse_value = c(base_rmse_therm_all$.estimate,rf_rmse_therm_all$.estimate,gbm_rmse_therm_all$.estimate,xgb_rmse_therm_all$.estimate)
therm_all_mpe_value = c(base_mpe_therm_all$.estimate,rf_mpe_therm_all$.estimate,gbm_mpe_therm_all$.estimate,xgb_mpe_therm_all$.estimate)

```

Modeling Code for KWH_ALL
```{r, message=FALSE, echo=FALSE, warning=FALSE}
##KWH MODELING_ALL
# split the dataset into train and test set
set.seed(17)
train_index_kwh_all = createDataPartition(kwh_all$KWH.PER.SQFT, p = 0.7, list = FALSE)
train_kwh_all = kwh_all[train_index_kwh_all, ]
test_kwh_all = kwh_all[-train_index_kwh_all, ]

# Baseline linear model
base_kwh_all = lm(KWH.PER.SQFT~ ., data = train_kwh_all)

# Randomforest
rf_kwh_all= randomForest(KWH.PER.SQFT~ ., data = train_kwh_all, ntree = 1000)

# GBM
trainControl_gbm_kwh_all = trainControl(method = "cv",  
                               number = 10)    
gbm_grid = expand.grid(interaction.depth = 1:8,  
                          n.trees = 1000,          
                          shrinkage = c(0.001, 0.005, 0.01, 0.05, 0.1),
                        n.minobsinnode = c(4,6,8,10))
gbm_kwh_all_train = train(KWH.PER.SQFT ~ ., data = train_kwh_all,
                    method = "gbm",
                    trControl = trainControl_gbm_kwh_all,
                    tuneGrid = gbm_grid,
                    verbose = FALSE,
                    metric = "RMSE",  
                    distribution = "gaussian")

gbm_kwh_all = gbm(KWH.PER.SQFT ~ ., data = train_kwh_all, distribution = "gaussian", 
                n.trees=1000,interaction.depth = gbm_kwh_all_train$bestTune$interaction.depth,shrinkage = gbm_kwh_all_train$bestTune$shrinkage,n.minobsinnode = gbm_kwh_all_train$bestTune$n.minobsinnode)


# XGboost
xkwh_all = xgb.DMatrix(data=as.matrix(train_kwh_all[,-3]),label=train_kwh_all[,3])
xgb_grid = expand.grid(max_depth = c(4, 6, 8, 10),  # Example depths
                           subsample = c(0.5, 0.7, 0.9),  # Example subsample rates
                           eta = c(0.005, 0.01, 0.05, 0.1),  # Example learning rates
                           stringsAsFactors = FALSE)

# Initialize variables to store the best parameters and lowest RMSE
best_params_kwh_all = list()
min_rmse_kwh_all = Inf
 
# Loop through the grid
for(i in 1:nrow(xgb_grid)) {
   params = list(
     booster = "gbtree",
     max_depth = xgb_grid$max_depth[i],
     subsample = xgb_grid$subsample[i],
     eta = xgb_grid$eta[i]
   )
   
# Perform cross-validation
   cv = xgb.cv(
     params = params,
     data = xkwh_all,
     nrounds = 10000,
     nfold = 5,  
     watchlist = list(train = xkwh_all),
     early_stopping_rounds = 10,
     metrics = "rmse",
     maximize = FALSE  
   )
   
# Check if this model has the lowest RMSE so far
   if(cv$evaluation_log$test_rmse_mean[cv$best_iteration] < min_rmse_kwh_all) {
     min_rmse_kwh_all = cv$evaluation_log$test_rmse_mean[cv$best_iteration]
     best_params_kwh_all = params
   }
 }

xgb_kwh_all = xgb.train(booster = "gbtree", max_depth = best_params_kwh_all$max_depth, subsample = best_params_kwh_all$subsample, eta = best_params_kwh_all$eta, data = xkwh_all,
                       nrounds = 10000, watchlist = list(train = xkwh_all),
                       early_stopping_rounds = 10,verbose = 0)

# compare performance of three tree models
ykwh_all = xgb.DMatrix(data=as.matrix(test_kwh_all[,-3]),label=test_kwh_all[,3])
base_predict_kwh_all = predict(base_kwh_all,newdata=test_kwh_all[,-3])
rf_predict_kwh_all = predict(rf_kwh_all,newdata=test_kwh_all[,-3])
gbm_predict_kwh_all = predict(gbm_kwh_all,newdata=test_kwh_all[,-3])
xgb_predict_kwh_all = predict(xgb_kwh_all,newdata=ykwh_all)
kwh_all_result = as.data.frame(cbind(test_kwh_all$KWH.PER.SQFT,base_predict_kwh_all,rf_predict_kwh_all,gbm_predict_kwh_all,xgb_predict_kwh_all))

base_rmse_kwh_all = yardstick::rmse(kwh_all_result,V1,base_predict_kwh_all)
rf_rmse_kwh_all = yardstick::rmse(kwh_all_result,V1,rf_predict_kwh_all)
gbm_rmse_kwh_all = yardstick::rmse(kwh_all_result,V1,gbm_predict_kwh_all)
xgb_rmse_kwh_all = yardstick::rmse(kwh_all_result,V1,xgb_predict_kwh_all)

base_mpe_kwh_all = mpe(kwh_all_result,V1,base_predict_kwh_all)
rf_mpe_kwh_all = mpe(kwh_all_result,V1,rf_predict_kwh_all)
gbm_mpe_kwh_all = mpe(kwh_all_result,V1,gbm_predict_kwh_all)
xgb_mpe_kwh_all = mpe(kwh_all_result,V1,xgb_predict_kwh_all)

kwh_all_rmse_value = c(base_rmse_kwh_all$.estimate,rf_rmse_kwh_all$.estimate,gbm_rmse_kwh_all$.estimate,xgb_rmse_kwh_all$.estimate)
kwh_all_mpe_value = c(base_mpe_kwh_all$.estimate,rf_mpe_kwh_all$.estimate,gbm_mpe_kwh_all$.estimate,xgb_mpe_kwh_all$.estimate)

```

##OnlyChicagoData Modeling

Modeling Code for THERM_ONLY_CHICAGO_DATA
```{r, message=FALSE, echo=FALSE, warning=FALSE}
##THERM MODELING_ONLY_CHICAGO_DATA
# split the dataset into train and test set
set.seed(17)
train_index_therm_only = createDataPartition(therm_only$THERMS.PER.SQFT, p = 0.7, list = FALSE)
train_therm_only = therm_only[train_index_therm_only, ]
test_therm_only = therm_only[-train_index_therm_only, ]

# Baseline linear model
base_therm_only = lm(THERMS.PER.SQFT~ ., data = train_therm_only)

# Randomforest
rf_therm_only= randomForest(THERMS.PER.SQFT~ ., data = train_therm_only, ntree = 1000)

# GBM
trainControl_gbm_therm_only = trainControl(method = "cv",  
                               number = 10)    
gbm_grid = expand.grid(interaction.depth = 1:8,  
                          n.trees = 1000,          
                          shrinkage = c(0.001, 0.005, 0.01, 0.05, 0.1),
                        n.minobsinnode = c(4,6,8,10))
gbm_therm_only_train = train(THERMS.PER.SQFT ~ ., data = train_therm_only,
                    method = "gbm",
                    trControl = trainControl_gbm_therm_only,
                    tuneGrid = gbm_grid,
                    verbose = FALSE,
                    metric = "RMSE",  
                    distribution = "gaussian")
gbm_therm_only = gbm(THERMS.PER.SQFT ~ ., data = train_therm_only, distribution = "gaussian", 
                n.trees=1000,interaction.depth = gbm_therm_only_train$bestTune$interaction.depth,shrinkage = gbm_therm_only_train$bestTune$shrinkage,n.minobsinnode = gbm_therm_only_train$bestTune$n.minobsinnode)


# XGboost
xtherm_only = xgb.DMatrix(data=as.matrix(train_therm_only[,-3]),label=train_therm_only[,3])
xgb_grid = expand.grid(max_depth = c(4, 6, 8, 10),  # Example depths
                           subsample = c(0.5, 0.7, 0.9),  # Example subsample rates
                           eta = c(0.005, 0.01, 0.05, 0.1),  # Example learning rates
                           stringsAsFactors = FALSE)

# Initialize variables to store the best parameters and lowest RMSE
best_params_therm_only = list()
min_rmse_therm_only = Inf
 
# Loop through the grid
for(i in 1:nrow(xgb_grid)) {
   params = list(
     booster = "gbtree",
     max_depth = xgb_grid$max_depth[i],
     subsample = xgb_grid$subsample[i],
     eta = xgb_grid$eta[i]
   )
   
# Perform cross-validation
   cv = xgb.cv(
     params = params,
     data = xtherm_only,
     nrounds = 10000,
     nfold = 5,  
     watchlist = list(train = xtherm_only),
     early_stopping_rounds = 10,
     metrics = "rmse",
     maximize = FALSE  
   )
   
# Check if this model has the lowest RMSE so far
   if(cv$evaluation_log$test_rmse_mean[cv$best_iteration] < min_rmse_therm_only) {
     min_rmse_therm_only = cv$evaluation_log$test_rmse_mean[cv$best_iteration]
     best_params_therm_only = params
   }
 }
 

xgb_therm_only = xgb.train(booster = "gbtree", max_depth = best_params_therm_only$max_depth, subsample = best_params_therm_only$subsample, eta = best_params_therm_only$eta, data = xtherm_only,
                       nrounds = 10000, watchlist = list(train = xtherm_only),
                       early_stopping_rounds = 10,verbose = 0)

# compare performance of three tree models
ytherm_only = xgb.DMatrix(data=as.matrix(test_therm_only[,-3]),label=test_therm_only[,3])
base_predict_therm_only = predict(base_therm_only,newdata=test_therm_only[,-3])
rf_predict_therm_only = predict(rf_therm_only,newdata=test_therm_only[,-3])
gbm_predict_therm_only = predict(gbm_therm_only,newdata=test_therm_only[,-3])
xgb_predict_therm_only = predict(xgb_therm_only,newdata=ytherm_only)
therm_only_result = as.data.frame(cbind(test_therm_only$THERMS.PER.SQFT,base_predict_therm_only,rf_predict_therm_only,gbm_predict_therm_only,xgb_predict_therm_only))

base_rmse_therm_only = yardstick::rmse(therm_only_result,V1,base_predict_therm_only)
rf_rmse_therm_only = yardstick::rmse(therm_only_result,V1,rf_predict_therm_only)
gbm_rmse_therm_only = yardstick::rmse(therm_only_result,V1,gbm_predict_therm_only)
xgb_rmse_therm_only = yardstick::rmse(therm_only_result,V1,xgb_predict_therm_only)

base_mpe_therm_only = mpe(therm_only_result,V1,base_predict_therm_only)
rf_mpe_therm_only = mpe(therm_only_result,V1,rf_predict_therm_only)
gbm_mpe_therm_only = mpe(therm_only_result,V1,gbm_predict_therm_only)
xgb_mpe_therm_only = mpe(therm_only_result,V1,xgb_predict_therm_only)

therm_only_rmse_value = c(base_rmse_therm_only$.estimate,rf_rmse_therm_only$.estimate,gbm_rmse_therm_only$.estimate,xgb_rmse_therm_only$.estimate)
therm_only_mpe_value = c(base_mpe_therm_only$.estimate,rf_mpe_therm_only$.estimate,gbm_mpe_therm_only$.estimate,xgb_mpe_therm_only$.estimate)

```

Modeling code for KWH_CHICAGO_ONLY
```{r, message=FALSE, echo=FALSE, warning=FALSE}
##KWH MODELING_ONLY
# split the dataset into train and test set
set.seed(17)
train_index_kwh_only = createDataPartition(kwh_only$KWH.PER.SQFT, p = 0.7, list = FALSE)
train_kwh_only = kwh_only[train_index_kwh_only, ]
test_kwh_only = kwh_only[-train_index_kwh_only, ]

# Baseline linear model
base_kwh_only = lm(KWH.PER.SQFT~ ., data = train_kwh_only)

# Randomforest
rf_kwh_only= randomForest(KWH.PER.SQFT~ ., data = train_kwh_only, ntree = 1000)

# GBM
trainControl_gbm_kwh_only = trainControl(method = "cv",  
                               number = 10)    
gbm_grid = expand.grid(interaction.depth = 1:8,  
                          n.trees = 1000,          
                          shrinkage = c(0.001, 0.005, 0.01, 0.05, 0.1),
                        n.minobsinnode = c(4,6,8,10))
gbm_kwh_only_train = train(KWH.PER.SQFT ~ ., data = train_kwh_only,
                    method = "gbm",
                    trControl = trainControl_gbm_kwh_only,
                    tuneGrid = gbm_grid,
                    verbose = FALSE,
                    metric = "RMSE",  
                    distribution = "gaussian")
gbm_kwh_only = gbm(KWH.PER.SQFT ~ ., data = train_kwh_only, distribution = "gaussian", 
                n.trees=1000,interaction.depth = gbm_kwh_only_train$bestTune$interaction.depth,shrinkage = gbm_kwh_only_train$bestTune$shrinkage,n.minobsinnode = gbm_kwh_only_train$bestTune$n.minobsinnode)


# XGboost
xkwh_only = xgb.DMatrix(data=as.matrix(train_kwh_only[,-3]),label=train_kwh_only[,3])
xgb_grid = expand.grid(max_depth = c(4, 6, 8, 10),  # Example depths
                           subsample = c(0.5, 0.7, 0.9),  # Example subsample rates
                           eta = c(0.005, 0.01, 0.05, 0.1),  # Example learning rates
                           stringsAsFactors = FALSE)

# Initialize variables to store the best parameters and lowest RMSE
best_params_kwh_only = list()
min_rmse_kwh_only = Inf
 
# Loop through the grid
for(i in 1:nrow(xgb_grid)) {
   params = list(
     booster = "gbtree",
     max_depth = xgb_grid$max_depth[i],
     subsample = xgb_grid$subsample[i],
     eta = xgb_grid$eta[i]
   )
   
# Perform cross-validation
   cv = xgb.cv(
     params = params,
     data = xkwh_only,
     nrounds = 10000,
     nfold = 5,  
     watchlist = list(train = xkwh_only),
     early_stopping_rounds = 10,
     metrics = "rmse",
     maximize = FALSE  
   )
   
# Check if this model has the lowest RMSE so far
   if(cv$evaluation_log$test_rmse_mean[cv$best_iteration] < min_rmse_kwh_only) {
     min_rmse_kwh_only = cv$evaluation_log$test_rmse_mean[cv$best_iteration]
     best_params_kwh_only = params
   }
 }


xgb_kwh_only = xgb.train(booster = "gbtree", max_depth = best_params_kwh_only$max_depth, subsample = best_params_kwh_only$subsample, eta = best_params_kwh_only$eta, data = xkwh_only,
                       nrounds = 10000, watchlist = list(train = xkwh_only),
                       early_stopping_rounds = 10,verbose = 0)

# compare performance of three tree models
ykwh_only = xgb.DMatrix(data=as.matrix(test_kwh_only[,-3]),label=test_kwh_only[,3])
base_predict_kwh_only = predict(base_kwh_only,newdata=test_kwh_only[,-3])
rf_predict_kwh_only = predict(rf_kwh_only,newdata=test_kwh_only[,-3])
gbm_predict_kwh_only = predict(gbm_kwh_only,newdata=test_kwh_only[,-3])
xgb_predict_kwh_only = predict(xgb_kwh_only,newdata=ykwh_only)
kwh_only_result = as.data.frame(cbind(test_kwh_only$KWH.PER.SQFT,base_predict_kwh_only,rf_predict_kwh_only,gbm_predict_kwh_only,xgb_predict_kwh_only))

base_rmse_kwh_only = yardstick::rmse(kwh_only_result,V1,base_predict_kwh_only)
rf_rmse_kwh_only = yardstick::rmse(kwh_only_result,V1,rf_predict_kwh_only)
gbm_rmse_kwh_only = yardstick::rmse(kwh_only_result,V1,gbm_predict_kwh_only)
xgb_rmse_kwh_only = yardstick::rmse(kwh_only_result,V1,xgb_predict_kwh_only)

base_mpe_kwh_only = mpe(kwh_only_result,V1,base_predict_kwh_only)
rf_mpe_kwh_only = mpe(kwh_only_result,V1,rf_predict_kwh_only)
gbm_mpe_kwh_only = mpe(kwh_only_result,V1,gbm_predict_kwh_only)
xgb_mpe_kwh_only = mpe(kwh_only_result,V1,xgb_predict_kwh_only)

kwh_only_rmse_value = c(base_rmse_kwh_only$.estimate,rf_rmse_kwh_only$.estimate,gbm_rmse_kwh_only$.estimate,xgb_rmse_kwh_only$.estimate)
kwh_only_mpe_value = c(base_mpe_kwh_only$.estimate,rf_mpe_kwh_only$.estimate,gbm_mpe_kwh_only$.estimate,xgb_mpe_kwh_only$.estimate)

```

```{r, message=FALSE, echo=FALSE, warning=FALSE}
## output the metrics for all the models
model_name = c("Linear","Random Forest", "Gradient Boosting Tree", "XGBoost")

therm_result = cbind(therm_all_rmse_value,therm_only_rmse_value,therm_all_mpe_value,therm_only_mpe_value)
therm_result = round(therm_result,3)
therm_result = cbind(model_name,therm_result)

kwh_result = cbind(kwh_all_rmse_value,kwh_only_rmse_value,kwh_all_mpe_value,kwh_only_mpe_value)
kwh_result = round(kwh_result,3)
kwh_result = cbind(model_name,kwh_result)

write.csv(therm_result, "../figures/therm_result.csv", row.names = FALSE)
write.csv(kwh_result, "../figures/kwh_result.csv", row.names = FALSE)

```
```{r, message=FALSE, echo=FALSE, warning=FALSE}
# output the importance ranking
imp_therm_all_gbm = summary(gbm_therm_all, plotit = FALSE)
imp_therm_all_gbm = head(imp_therm_all_gbm,15)
imp_therm_all_gbm[,2] = round(imp_therm_all_gbm[,2],3)

imp_kwh_all_gbm = summary(gbm_kwh_all, plotit = FALSE)
imp_kwh_all_gbm = head(imp_kwh_all_gbm,15)
imp_kwh_all_gbm[,2] = round(imp_kwh_all_gbm[,2],3)

imp_therm_all_xgb = xgb.importance(model = xgb_therm_all)[,-c(3,4)]
imp_therm_all_xgb = head(imp_therm_all_xgb,15)
imp_therm_all_xgb[,2] = round(imp_therm_all_xgb[,2],3)

imp_kwh_all_xgb = xgb.importance(model = xgb_kwh_all)[,-c(3,4)]
imp_kwh_all_xgb = head(imp_kwh_all_xgb,15)
imp_kwh_all_xgb[,2] = round(imp_kwh_all_xgb[,2],3)

therm_importance_result = cbind(imp_therm_all_xgb,imp_therm_all_gbm)
colnames(therm_importance_reuslt) = c("XGBoost Therm Feature", "XGBoost Therm Importance", "GBDT Therm Feature", "GBDT Therm Importance")

kwh_importance_result = cbind(imp_kwh_all_xgb,imp_kwh_all_gbm)
colnames(kwh_importance_reuslt) = c("XGBoost KWH Feature", "XGBoost KWH Importance", "GBDT KWH Feature", "GBDT KWH Importance")

write.csv(therm_importance_result, "../figures/therm_importance_result.csv", row.names = FALSE)
write.csv(kwh_importance_result, "../figures/kwh_importance_result.csv", row.names = FALSE)
```

```{r, message=FALSE, echo=FALSE, warning=FALSE}
Dtherm_all = xgb.DMatrix(data=as.matrix(therm_all[,-3]),label=therm_all[,3])
Dxgb_predict_therm_all = predict(xgb_therm_all,newdata=Dtherm_all)
therm_residual = therm_all$THERMS.PER.SQFT - Dxgb_predict_therm_all
Dtherm_all_result = as.data.frame(cbind(therm_residual,therm_all$Longitude,therm_all$Latitude))
colnames(Dtherm_all_result) = c("Rediduals for therm","Longitude","Latitude")

Dkwh_all = xgb.DMatrix(data=as.matrix(kwh_all[,-3]),label=kwh_all[,3])
Dxgb_predict_kwh_all = predict(xgb_kwh_all,newdata=Dkwh_all)
kwh_residual = kwh_all$KWH.PER.SQFT - Dxgb_predict_kwh_all
Dkwh_all_result = as.data.frame(cbind(kwh_residual,kwh_all$Longitude,kwh_all$Latitude))
colnames(Dkwh_all_result) = c("Rediduals for therm","Longitude","Latitude")

write.csv(Dtherm_all_result, "../figures/Dtherm_all_result.csv", row.names = FALSE)
write.csv(Dkwh_all_result, "../figures/Dkwh_all_result.csv", row.names = FALSE)
```
